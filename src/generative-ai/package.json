{
        "model_type": "llama-3.2-1b-instruct-q8_0",
        "hidden_size": 1024,
        "num_attention_heads": 16,
        "num_hidden_layers": 24,
        "vocab_size": 32000
}
